<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content=""><title>Introduction to Topic Modeling | Statistical Data Visualization</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/languages/r.min.js"></script>
<script>hljs.highlightAll();</script>

  <meta name="keywords" content="">
  <meta name="description" content=""><link rel="stylesheet" href="/stat679_notes/assets/main.css?v=0.3.3" />
<script src="/stat679_notes/assets/main.js?v=0.3.3" defer></script><link rel="stylesheet" href="/stat679_notes/assets/css/tomorrow.css" />
<script src="/stat679_notes/assets/js/highlight.js"></script><script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><script src="https://unpkg.com/mermaid@7.1.0/dist/mermaid.min.js" defer></script></head>
<body class="body-post">
    <a href="/stat679_notes/" class="logo"><h1>Statistical Data Visualization</h1>
</a><main class="post__wrapper"><div class="post__top_navs clearfix">
    <nav class="post__archive_path"><a href="" id="archiveBtn">
        <div class="post__archive_icon">
          <svg width="40" height="40">
            <circle class="circle-progress" r="18" cy="20" cx="20"  stroke-linejoin="round" stroke-linecap="round" />
          </svg>
          <span class="post__archive_icon"></span>
        </div>
        Statistical Data Visualization
      </a>
    </nav>
  </div>
  <article class="post">
    <header class="post__header">
      <h1 class="post__title">Introduction to Topic Modeling</h1>
      <div class="post__meta">
        <time>2022-06-02 00:00</time>
      </div>
    </header>
    <div class="post__content content">
      <p><em>Quantitative descriptions of document topics</em></p>

<p><a href="https://github.com/krisrs1128/stat679_code/tree/main/examples/week12/week12-1.Rmd">Code</a>,
<a href="">Recording</a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library(tidyverse)
library(tidytext)
library(topicmodels)
library(gutenbergr) # devtools::install_github("ropensci/gutenbergr")
</code></pre></div></div>

<ol>
  <li>
    <p>Topic modeling is a type of dimensionality reduction method that is
especially useful for high-dimensional count matrices. For example,
it can be applied to,</p>

    <ul>
      <li>Text data analysis, where each row is a document and each column
is a word. The <em>i**j</em> entry contains the count of word <em>j</em> word
in the document <em>i</em>.</li>
      <li>Gene expression analysis, where each row is a biological sample
and each column is a gene. The <em>i**j</em> entry measures the amount
of gene <em>j</em> expressed in sample <em>i</em>.</li>
    </ul>

    <p>This week, we’ll specifically focus on the application to text data,
since otherwise, we’ve covered relatively few visualization
techniques that can be applied to this (very common) type of data.
For the rest of these lectures, we’ll refer to samples as documents
and features as words, even though these methods can be used more
generally.</p>
  </li>
  <li>
    <p>These models are useful to know about because they provide a
compromise between clustering and PCA.</p>

    <ul>
      <li>In clustering, each document would have to be assigned to a
single topic. In contrast, topic models allow each document to
partially belong to several topics simultaneously. In this
sense, they are more suitable when data do not belong to
distinct, clearly-defined clusters.</li>
      <li>PCA is also appropriate when the data vary continuously, but it
does not provide any notion of clusters. In contrast, topic
models estimate <em>K</em> topics, which are analogous to a cluster
centroids (though documents are typically a mix of several
centroids).</li>
    </ul>
  </li>
  <li>
    <p>Without going into mathematical detail, topic models perform
dimensionality reduction by supposing, (i) each document is a
mixture of topics and (ii) each topic is a mixture of words. For
(i), consider modeling a collection of newspaper articles. A set of
articles might belong primarily to the “politics” topic, and others
to the “business” topic. Articles that describe a monetary policy in
the federal reserve might belong partially to both the “politics”
and the “business” topic. For (ii), consider the difference in words
that would appear in politics and business articles. Articles about
politics might frequently include words like “congress” and “law,”
but only rarely words like “stock” and “trade.”</p>

    <p align="center">

<img src="/stat679_notes/assets/week12-1/topics_overview.png" width="700" />

</p>
  </li>
  <li>
    <p>A document is a mixture of topics, with more words coming from the
topics that it is close to. More precisely, a document that is very
close to a particular topic has a word distribution just like that
topic. A document that is intermediate between two topics has a word
distribution that mixes between both topics.</p>

    <p align="center">

<img src="/stat679_notes/assets/week12-1/LDA-f3.png" width="250" />

</p>
  </li>
  <li>
    <p>Let’s see how to fit a topic model in R. We will use LDA as
implemented in the <code class="language-plaintext highlighter-rouge">topicmodels</code> package, which expects input to be
structured as a <code class="language-plaintext highlighter-rouge">DocumentTermMatrix</code>, a special type of matrix that
stores the counts of words (columns) across documents (rows). In
practice, most of the effort required to fit a topic model goes into
transforming the raw data into a suitable <code class="language-plaintext highlighter-rouge">DocumentTermMatrix</code>.</p>
  </li>
  <li>
    <p>To illustrate this process, let’s consider the “Great Library Heist”
example from the reading. We imagine that a thief has taken four
books — Great Expectations, Twenty Thousand Leagues Under The Sea,
War of the Worlds, and Pride &amp; Prejudice — and torn all the chapters
out. We are left with pieces of isolated pieces of text and have to
determine from which book they are from. The block below downloads
all the books into an R object.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>titles &lt;- c("Twenty Thousand Leagues under the Sea",
        "The War of the Worlds",
        "Pride and Prejudice", 
        "Great Expectations")
books &lt;- gutenberg_works(title %in% titles) %&gt;%
  gutenberg_download(meta_fields = "title")
books

## # A tibble: 53,724 × 3
##    gutenberg_id text                    title                
##           &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt;                
##  1           36 "cover "                The War of the Worlds
##  2           36 ""                      The War of the Worlds
##  3           36 ""                      The War of the Worlds
##  4           36 ""                      The War of the Worlds
##  5           36 ""                      The War of the Worlds
##  6           36 "The War of the Worlds" The War of the Worlds
##  7           36 ""                      The War of the Worlds
##  8           36 "by H. G. Wells"        The War of the Worlds
##  9           36 ""                      The War of the Worlds
## 10           36 ""                      The War of the Worlds
## # … with 53,714 more rows
</code></pre></div>    </div>
  </li>
  <li>
    <p>Since we imagine that the word distributions are not equal across
the books, topic modeling is a reasonable approach for discovering
the books associated with each chapter. Let’s start by simulating
the process of tearing the chapters out. We split the raw texts
anytime the word “Chapter” appears. We will keep track of the book
names for each chapter, but this information is not passed into the
topic modeling algorithm.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>by_chapter &lt;- books %&gt;%
  group_by(title) %&gt;%
  mutate(
    chapter = cumsum(str_detect(text, regex("chapter", ignore_case = TRUE)))
  ) %&gt;%
  group_by(title, chapter) %&gt;%
  mutate(n = n()) %&gt;%
  filter(n &gt; 5) %&gt;%
  ungroup() %&gt;%
  unite(document, title, chapter)
</code></pre></div>    </div>
  </li>
  <li>
    <p>As it is, the text data are long character strings, giving actual
text from the novels. To fit LDA, we only need counts of each word
within each chapter – the algorithm throws away information related
to word order. To derive word counts, we first split the raw text
into separate words using the <code class="language-plaintext highlighter-rouge">unest_tokens</code> function in the
tidytext package. Then, we can count the number of times each word
appeared in each document using count, a shortcut for the usual
<code class="language-plaintext highlighter-rouge">group_by</code> and <code class="language-plaintext highlighter-rouge">summarize(n = n())</code> pattern.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>word_counts &lt;- by_chapter %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words) %&gt;%
  count(document, word)
</code></pre></div>    </div>
  </li>
  <li>
    <p>These words counts are still not in a format compatible with
conversion to a <code class="language-plaintext highlighter-rouge">DocumentTermMatrix</code>. The issue is that the
<code class="language-plaintext highlighter-rouge">DocumentTermMatrix</code> expects words to be arranged along columns, but
currently they are stored across rows. The line below converts the
original “long” word counts into a “wide” DocumentTermMatrix in one
step. Across these 4 books, we have 65 chapters and a vocabulary of
size 18325.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>chapters_dtm &lt;- word_counts %&gt;%
  cast_dtm(document, word, n)
</code></pre></div>    </div>
  </li>
  <li>
    <p>Once the data are in this format, we can use the LDA function to fit
a topic model. We choose <em>K</em> = 4 topics because we expect that each
topic will match a book. Different hyperparameters can be set using
the control argument. There are two types of outputs produced by the
LDA model: the topic word distributions (for each topic, which words
are common?) and the document-topic memberships (from which topics
does a document come from?). For visualization, it will be easiest
to extract these parameters using the tidy function, specifying
whether we want the topics (beta) or memberships (gamma).</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>chapters_lda &lt;- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda

## A LDA_VEM topic model with 4 topics.

topics &lt;- tidy(chapters_lda, matrix = "beta")
memberships &lt;- tidy(chapters_lda, matrix = "gamma")
</code></pre></div>    </div>
  </li>
  <li>
    <p>This tidy approach is preferable to extracting the parameters
directly from the fitted model (e.g., using <code class="language-plaintext highlighter-rouge">chapters_lda@gamma</code>)
because it ensures the output is a tidy data.frame, rather than a
matrix. Tidy data.frames are easier to visualize using ggplot2.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># highest weight words per topic
topics %&gt;%
  arrange(topic, -beta)

## # A tibble: 74,976 × 3
##    topic term          beta
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1     1 captain    0.0153 
##  2     1 _nautilus_ 0.0126 
##  3     1 sea        0.00907
##  4     1 nemo       0.00863
##  5     1 ned        0.00789
##  6     1 conseil    0.00676
##  7     1 water      0.00599
##  8     1 land       0.00598
##  9     1 sir        0.00485
## 10     1 day        0.00365
## # … with 74,966 more rows

# topic memberships per document
memberships %&gt;%
  arrange(document, topic)

## # A tibble: 780 × 3
##    document               topic     gamma
##    &lt;chr&gt;                  &lt;int&gt;     &lt;dbl&gt;
##  1 Great Expectations_0       1 0.00282  
##  2 Great Expectations_0       2 0.461    
##  3 Great Expectations_0       3 0.00282  
##  4 Great Expectations_0       4 0.533    
##  5 Great Expectations_100     1 0.000424 
##  6 Great Expectations_100     2 0.000424 
##  7 Great Expectations_100     3 0.999    
##  8 Great Expectations_100     4 0.000424 
##  9 Great Expectations_101     1 0.0000140
## 10 Great Expectations_101     2 0.0000140
## # … with 770 more rows
</code></pre></div>    </div>
  </li>
</ol>

    </div>
  </article></main><script>hljs.initHighlightingOnLoad();</script><footer class="site-footer">
  © 2022<a href="/stat679_notes/">Statistical Data Visualization</a>. Theme<a href="https://github.com/erlzhang/jekyll-theme-persephone" target="_blank">Persephone</a>
</footer>

  </body>
</html>
