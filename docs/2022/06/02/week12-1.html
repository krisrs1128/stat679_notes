<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content=""><title>Introduction to Topic Modeling | Statistical Data Visualization</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/languages/r.min.js"></script>
<script>hljs.highlightAll();</script>

  <meta name="keywords" content="">
  <meta name="description" content=""><link rel="stylesheet" href="/stat679_notes/assets/main.css?v=0.3.3" />
<script src="/stat679_notes/assets/main.js?v=0.3.3" defer></script><link rel="stylesheet" href="/stat679_notes/assets/css/tomorrow.css" />
<script src="/stat679_notes/assets/js/highlight.js"></script><script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><script src="https://unpkg.com/mermaid@7.1.0/dist/mermaid.min.js" defer></script></head>
<body class="body-post">
    <a href="/stat679_notes/" class="logo"><h1>Statistical Data Visualization</h1>
</a><main class="post__wrapper"><div class="post__top_navs clearfix">
    <nav class="post__archive_path"><a href="" id="archiveBtn">
        <div class="post__archive_icon">
          <svg width="40" height="40">
            <circle class="circle-progress" r="18" cy="20" cx="20"  stroke-linejoin="round" stroke-linecap="round" />
          </svg>
          <span class="post__archive_icon"></span>
        </div>
        Statistical Data Visualization
      </a>
    </nav>
  </div>
  <article class="post">
    <header class="post__header">
      <h1 class="post__title">Introduction to Topic Modeling</h1>
      <div class="post__meta">
        <time>2022-06-02 00:00</time>
      </div>
    </header>
    <div class="post__content content">
      <p><em>Short description</em></p>

<ol>
  <li>
    <p>Topic modeling is a type of dimensionality reduction method that is
especially useful for high-dimensional count matrices. For example,
it can be applied to,</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> * Text data analysis, where each row is a document and each column is a word.
 The ijth entry contains the count of the jth word in the ith document.
 * Gene expression analysis, where each row is a biological sample and each
 column is a gene. The $ij^{th}$ entry measures the amount of gene j expressed in
 sample i.
</code></pre></div>    </div>

    <p>This week, we’ll specifically focus on the application to text data,
since otherwise, we’ve covered relatively few visualization
techniques that can be applied to this (very common) type of data.
For the rest of these lectures, we’ll refer to samples as documents
and features as words, even though these methods can be used more
generally.</p>
  </li>
  <li>
    <p>These models are useful to know about because they provide a
compromise between clustering and PCA.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> * In clustering, each document would have to be assigned to a single topic. In
 contrast, topic models allow each document to partially belong to several
 topics simultaneously. In this sense, they are more suitable when data do not
 belong to distinct, clearly-defined clusters.
 * PCA is also appropriate when the data vary continuously, but it does not
 provide any notion of clusters. In contrast, topic models estimate K topics,
 which are analogous to a cluster centroids (though documents are typically a
 mix of several centroids).
</code></pre></div>    </div>
  </li>
  <li>
    <p>Without going into mathematical detail, topic models perform
dimensionality reduction by supposing, (i) each document is a
mixture of topics and (ii) each topic is a mixture of words. For
(i), consider modeling a collection of newspaper articles. A set of
articles might belong primarily to the “politics” topic, and others
to the “business” topic. Articles that describe a monetary policy in
the federal reserve might belong partially to both the “politics”
and the “business” topic. For (ii), consider the difference in words
that would appear in politics and business articles. Articles about
politics might frequently include words like “congress” and “law,”
but only rarely words like “stock” and “trade.”</p>
  </li>
  <li>
    <p>A document is a mixture of topics, with more words coming from the
topics that it is close to. More precisely, a document that is very
close to a particular topic has a word distribution just like that
topic. A document that is intermediate between two topics has a word
distribution that mixes between both topics.</p>
  </li>
  <li>
    <p>Let’s see how to fit a topic model in R. We will use LDA as
implemented in the topicmodels package, which expects input to be
structured as a DocumentTermMatrix, a special type of matrix that
stores the counts of words (columns) across documents (rows). In
practice, most of the effort required to fit a topic model goes into
transforming the raw data into a suitable DocumentTermMatrix.</p>
  </li>
  <li>
    <p>To illustrate this process, let’s consider the “Great Library Heist”
example from the reading. We imagine that a thief has taken four
books — Great Expectations, Twenty Thousand Leagues Under The Sea,
War of the Worlds, and Pride &amp; Prejudice — and torn all the chapters
out. We are left with pieces of isolated pieces of text and have to
determine from which book they are from. The block below downloads
all the books into an R object.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    titles &lt;- c("Twenty Thousand Leagues under the Sea",
                "The War of the Worlds",
                "Pride and Prejudice", 
                "Great Expectations")
    books &lt;- gutenberg_works(title %in% titles) %&gt;%
      gutenberg_download(meta_fields = "title")
    books
</code></pre></div>    </div>
  </li>
  <li>
    <p>Since we imagine that the word distributions are not equal across
the books, topic modeling is a reasonable approach for discovering
the books associated with each chapter. Let’s start by simulating
the process of tearing the chapters out. We split the raw texts
anytime the word “Chapter” appears. We will keep track of the book
names for each chapter, but this information is not passed into the
topic modeling algorithm.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    by_chapter &lt;- books %&gt;%
      group_by(title) %&gt;%
      mutate(
        chapter = cumsum(str_detect(text, regex("chapter", ignore_case = TRUE)))
      ) %&gt;%
      group_by(title, chapter) %&gt;%
      mutate(n = n()) %&gt;%
      filter(n &gt; 5) %&gt;%
      ungroup() %&gt;%
      unite(document, title, chapter)
</code></pre></div>    </div>
  </li>
  <li>
    <p>As it is, the text data are long character strings, giving actual
text from the novels. To fit LDA, we only need counts of each word
within each chapter – the algorithm throws away information related
to word order. To derive word counts, we first split the raw text
into separate words using the <code class="language-plaintext highlighter-rouge">unest_tokens</code> function in the
tidytext package. Then, we can count the number of times each word
appeared in each document using count, a shortcut for the usual
<code class="language-plaintext highlighter-rouge">group_by</code> and <code class="language-plaintext highlighter-rouge">summarize(n = n())</code> pattern.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>word_counts &lt;- by_chapter %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words) %&gt;%
  count(document, word)
</code></pre></div>    </div>
  </li>
  <li>
    <p>These words counts are still not in a format compatible with
conversion to a DocumentTermMatrix. The issue is that the
DocumentTermMatrix expects words to be arranged along columns, but
currently they are stored across rows. The line below converts the
original “long” word counts into a “wide” DocumentTermMatrix in one
step. Across these 4 books, we have 65 chapters and a vocabulary of
size</p>
  </li>
  <li></li>
</ol>

<!-- -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```r
    chapters_dtm &lt;- word_counts %&gt;%
      cast_dtm(document, word, n)
```
</code></pre></div></div>

<ol>
  <li>
    <p>Once the data are in this format, we can use the LDA function to fit
a topic model. We choose K=4 topics because we expect that each
topic will match a book. Different hyperparameters can be set using
the control argument. There are two types of outputs produced by the
LDA model: the topic word distributions (for each topic, which words
are common?) and the document-topic memberships (from which topics
does a document come from?). For visualization, it will be easiest
to extract these parameters using the tidy function, specifying
whether we want the topics (beta) or memberships (gamma).</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>chapters_lda &lt;- LDA(chapters_dtm, k = 4, control = list(seed = 1234))
chapters_lda

topics &lt;- tidy(chapters_lda, matrix = "beta")
    memberships &lt;- tidy(chapters_lda, matrix = "gamma")
</code></pre></div>    </div>
  </li>
  <li>
    <p>This tidy approach is preferable to extracting the parameters
directly from the fitted model (e.g., using <code class="language-plaintext highlighter-rouge">chapters_lda@gamma</code>)
because it ensures the output is a tidy data.frame, rather than a
matrix. Tidy data.frames are easier to visualize using ggplot2.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    # highest weight words per topic
    topics %&gt;%
      arrange(topic, -beta)

    # topic memberships per document
    memberships %&gt;%
      arrange(document, topic)
</code></pre></div>    </div>
  </li>
</ol>

    </div>
  </article></main><script>hljs.initHighlightingOnLoad();</script><footer class="site-footer">
  © 2022<a href="/stat679_notes/">Statistical Data Visualization</a>. Theme<a href="https://github.com/erlzhang/jekyll-theme-persephone" target="_blank">Persephone</a>
</footer>

  </body>
</html>
