<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content=""><title>Principal Components Analysis | Statistical Data Visualization</title><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/languages/r.min.js"></script>
<script>hljs.highlightAll();</script>

  <meta name="keywords" content="">
  <meta name="description" content=""><link rel="stylesheet" href="/stat679_notes/assets/main.css?v=0.3.3" />
<script src="/stat679_notes/assets/main.js?v=0.3.3" defer></script><link rel="stylesheet" href="/stat679_notes/assets/css/tomorrow.css" />
<script src="/stat679_notes/assets/js/highlight.js"></script><script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><script src="https://unpkg.com/mermaid@7.1.0/dist/mermaid.min.js" defer></script></head>
<body class="body-post">
    <a href="/stat679_notes/" class="logo"><h1>Statistical Data Visualization</h1>
</a><main class="post__wrapper"><div class="post__top_navs clearfix">
    <nav class="post__archive_path"><a href="" id="archiveBtn">
        <div class="post__archive_icon">
          <svg width="40" height="40">
            <circle class="circle-progress" r="18" cy="20" cx="20"  stroke-linejoin="round" stroke-linecap="round" />
          </svg>
          <span class="post__archive_icon"></span>
        </div>
        Statistical Data Visualization
      </a>
    </nav>
  </div>
  <article class="post">
    <header class="post__header">
      <h1 class="post__title">Principal Components Analysis</h1>
      <div class="post__meta">
        <time>2022-06-02 00:00</time>
      </div>
    </header>
    <div class="post__content content">
      <p><em>Linear dimensionality reduction with PCA</em></p>

<ol>
  <li>
    <p>For low-dimensional data, we could visually encode all the features
in our data directly, either using properties of marks or through
faceting. In high-dimensional data, this is no longer possible.
However, though there are many features associated with each
observation, it may still be possible to organize samples across a
smaller number of meaningful, derived features. In the next week,
we’ll explore a few ways of partially automating the search for
relevant features.</p>
  </li>
  <li>
    <p>An important special case for dimensionality reduction emerges when
we make the following assumptions about the set of derived features,</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> * Features that are linear combinations of the raw input columns.
 * Features that are orthogonal to one another.
 * Features that have high variance.
</code></pre></div>    </div>
  </li>
  <li>
    <p>Why would we want features to have these properties?</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> * Restricting to linear combinations allows for an analytical solution. We
 will relax this requirement when discussing UMAP.
 * Orthogonality means that the derived features will be uncorrelated with one
 another. This is a nice property, because it would be wasteful if features
 were redundant.
 * High variance is desirable because it means we preserve more of the
 essential structure of the underlying data. For example, if you look at this
 2D representation of a 3D object, it’s hard to tell what it is,

 (img)

 But when viewing an alternative reduction which has higher variance…

 (img)
</code></pre></div>    </div>
  </li>
  <li>
    <p>Principal Components Analysis (PCA) is the optimal dimensionality
reduction under these three restrictions, in the sense that it finds
derived features with the highest variance. Formally, PCA finds a
matrix Φ∈ℝD×K and a set of vector zi∈ℝK such that xi≈Φzi for all i.
The columns of Φ are called principal components, and they specify
the structure of the derived linear features. The vector zi is
called the score of xi with respect to these components. The top
component explains the most variance, the second captures the next
most, and so on. Geometrically, the columns of Φ span a plane that
approximates the data. The zi provide coordinates of points
projected onto this plane.</p>
  </li>
  <li>
    <p>In R, PCA can be conveniently implemented using the tidymodels
package. The dataset below contains properties of a variety of
cocktails, from the Boston Bartender’s guide. The first two columns
are qualitative descriptors, while the rest give numerical
ingredient information.</p>
  </li>
  <li>
    <p>The <code class="language-plaintext highlighter-rouge">pca_rec</code> object below defines a tidymodels recipe for
performing PCA. Computation of the lower-dimensional representation
is deferred until prep() is called. This delineation between
workflow definition and execution helps clarify the overall
workflow, and it is typical of the tidymodels package.</p>
  </li>
  <li>
    <p>We can tidy each element of the workflow object. Since PCA was the
second step in the workflow, the PCA components can be obtained by
calling tidy with the argument “2.” The scores of each sample with
respect to these components can be extracted using juice. The amount
of variance explained by each dimension is also given by tidy, but
with the argument type = “variance”.</p>
  </li>
  <li>
    <p>We can interpret components by looking at the linear coefficients of
the variables used to define them. From the plot below, we see that
the first PC mostly captures variation related to whether the drink
is made with powdered sugar or simple syrup. Drinks with high values
of PC1 are usually to be made from simple syrup, those with low
values of PC1 are usually made from powdered sugar. From the two
largest bars in PC2, we can see that it highlights the vermouth
vs. non-vermouth distinction.</p>
  </li>
  <li>
    <p>It is often easier read the components when the bars are sorted
according to their magnitude. The usual ggplot approach to
reordering axes labels, using either reorder() or releveling the
associated factor, will reorder all the facets in the same way. If
we want to reorder each facet on its own, we can use the
<code class="language-plaintext highlighter-rouge">reorder_within</code> function coupled with <code class="language-plaintext highlighter-rouge">scale_*_reordered</code>, both
from the tidytext package.</p>
  </li>
  <li>
    <p>Next, we can visualize the scores of each sample with respect to
these components. The plot below shows (zi1,zi2). Suppose that the
columns of Φ are φ1,…,φK. Then, since xi≈φ1zi1+φ2zi2, the samples
have large values for variables with large component values in the
coordinate directions where zi is farther along.</p>
  </li>
  <li>
    <p>We conclude with some characteristics of PCA, which can guide the
choice between alternative dimensionality reduction methods.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> * Global structure: Since PCA is looking for high-variance overall, it tends
 to focus on global structure.
 * Linear: PCA can only consider linear combinations of the original features.
 If we expect nonlinear features to be more meaningful, then another approach
 should be considered.
 * Interpretable features: The PCA components exactly specify how to construct
 each of the derived features.
 * Fast: Compared to most dimensionality reduction methods, PCA is quite fast.
 Further, it is easy to implement approximate versions of PCA that scale to
 very large datasets.
 * Deterministic: Some embedding algorithms perform an optimization process,
 which means there might be some variation in the results due to randomness in
 the optimization. In contrast, PCA is deterministic, with the components being
 unique up to sign (i.e., you could reflect the components across an axis, but
 that is the most the results might change).
</code></pre></div>    </div>
  </li>
</ol>

    </div>
  </article></main><script>hljs.initHighlightingOnLoad();</script><footer class="site-footer">
  © 2022<a href="/stat679_notes/">Statistical Data Visualization</a>. Theme<a href="https://github.com/erlzhang/jekyll-theme-persephone" target="_blank">Persephone</a>
</footer>

  </body>
</html>
